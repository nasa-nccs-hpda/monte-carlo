{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to open terminal and run: conda activate ilab-tensorflow to import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/explore/nobackup/people/gtamkin/dev/AGB/mpf-model-factories/MultiPathFusion')\n",
    "sys.path.append('/explore/nobackup/people/gtamkin/dev/AGB/monte-carlo/monte-carlo')\n",
    "#sys.path.append('/explore/nobackup/people/gtamkin/dev/AGB/mpf-model-factories/MultiPathFusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 10:10:52.148198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 10:10:52.328308: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from multi_path_fusion.src.training.train_gt import train\n",
    "\n",
    "from multi_path_fusion.src.utils.mlflow_helpers import setup_mlflow, log_params\n",
    "from multi_path_fusion.src.utils.data_generator_helpers import load_data_generator\n",
    "from multi_path_fusion.src.utils.model_helpers import get_model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import rasterio\n",
    "import pickle\n",
    "from deepdiff import DeepDiff\n",
    "from pprint import pprint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def configure_virtual_cpus(ncpu):\n",
    "#   phy_devices = tf.config.list_physical_devices('CPU')\n",
    "#   tf.config.set_logical_device_configuration(phy_devices[0], [\n",
    "#         tf.config.LogicalDeviceConfiguration(),\n",
    "#     ] * ncpu)\n",
    "\n",
    "# configure_virtual_cpus(8)\n",
    "# DEVICES = [f'CPU:{i}' for i in range(8)]\n",
    "\n",
    "# tf.config.list_logical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/panfs/ccds02/home/gtamkin/dev/AGB/monte-carlo/monte-carlo/tests/exp_7bands.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    models_config = config[\"models\"]\n",
    "    data_generator_config = config[\"data_generator\"]\n",
    "    mlflow_config = config[\"mlflow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in band: 2\n",
      "reading in band: 16\n",
      "reading in band: 2\n",
      "reading in band: 16\n",
      "reading in band: 2\n",
      "reading in band: 16\n"
     ]
    }
   ],
   "source": [
    "# load train, validate, and test data\n",
    "train_generator = load_data_generator(data_generator_config, 'train')\n",
    "validate_generator = load_data_generator(data_generator_config, 'validate')\n",
    "test_generator = load_data_generator(data_generator_config, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRACKING_URI': '/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands_load_model', 'EXPERIMENT_NAME': 'Exp_7bands_stats[18,14,9,3,20,8,10]'}\n",
      "{'branch_inputs': [{'branch_files': [{'mlbs_year_filepath': 'MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices.tif', 'bands1': [18, 14, 9, 3, 20, 8, 10], 'bands2': [17, 4, 1, 15, 6, 7, 11], 'bands3': [16, 13, 12, 2, 5, 19, 21], 'bands': [2, 16], 'bands7': [11, 16, 9, 10, 8], 'bands6': [12, 2, 4, 21, 13, 14, 17, 19, 1, 7, 18, 16, 20, 6, 15, 9, 5, 3, 8, 10, 11], 'bands5': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]}]}], 'data_path': '/explore/nobackup/projects/ilab/data/AGB/test/beta_pmm/', 'dataset_width': 1000, 'dataset_height': 1000, 'patch_width': 1, 'patch_height': 1, 'batch_size': 50, 'full_hyperspectral': False, 'num_bands': 5, 'truth_file_a': 'MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_FSDGtif_CHM_warp.tif', 'truth_file_b': 'MLBS_2021_541567.6_4136443.0_542567.6_4137443.0/MLBS_2021_FSDGtif_CHM_warp.tif', 'year_a': 2018, 'year_b': 2021, 'height_inc': True, 'randomize_pixels': True, 'scale_data_method': 'minmax', 'split_tuple': [80, 10, 10], 'bins': [-0.5, -0.1, 0.1, 0.5]}\n",
      "[{'model_name': 'MLP_7bands_2_epochs', 'model_type': 'Sequential', 'model_description': 'Testing the Keras functionality after this whole architecture redesign', 'layers': [{'type': 'Dense', 'units': 5, 'return_sequences': True, 'activation': 'gelu', 'kernel_initializer': 'he_normal', 'use_bias': True}, {'type': 'Dense', 'units': 5, 'activation': 'softmax'}], 'compile_options': {'optimizer': 'adam', 'loss': 'sparse_categorical_crossentropy'}, 'training_options': {'epochs': 1}, 'callbacks': {'EarlyStopping': {'monitor': 'val_loss', 'patience': 20}}}]\n"
     ]
    }
   ],
   "source": [
    "print(mlflow_config)\n",
    "print(data_generator_config)\n",
    "print(models_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'MLP_7bands_2_epochs', 'model_type': 'Sequential', 'model_description': 'Testing the Keras functionality after this whole architecture redesign', 'layers': [{'type': 'Dense', 'units': 5, 'return_sequences': True, 'activation': 'gelu', 'kernel_initializer': 'he_normal', 'use_bias': True}, {'type': 'Dense', 'units': 5, 'activation': 'softmax'}], 'compile_options': {'optimizer': 'adam', 'loss': 'sparse_categorical_crossentropy'}, 'training_options': {'epochs': 1}, 'callbacks': {'EarlyStopping': {'monitor': 'val_loss', 'patience': 20}}} {'TRACKING_URI': '/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands_load_model', 'EXPERIMENT_NAME': 'Exp_7bands_stats[18,14,9,3,20,8,10]', 'EXPERIMENT_ID': '533710230248440706'}\n"
     ]
    }
   ],
   "source": [
    "model_config = models_config[0]\n",
    "\n",
    "mlflow_config = setup_mlflow(mlflow_config)\n",
    "print(model_config, mlflow_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING RUN: 51db891307ea46f98b9537b458177419\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/15 11:18:55 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'multi_path_fusion.src.data_generators.mpf_h_data_generator.MPF_H_DataGenerator'>. Dataset logging skipped.\n",
      "2024/01/15 11:18:55 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'multi_path_fusion.src.data_generators.mpf_h_data_generator.MPF_H_DataGenerator'>. Dataset logging skipped.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/dense_2/BiasAdd' defined at (most recent call last):\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3216345/2124766832.py\", line 34, in <module>\n      history = model_factory.train_model(model, train_generator, validate_generator, model_config)\n    File \"/explore/nobackup/people/gtamkin/dev/AGB/mpf-model-factories/MultiPathFusion/multi_path_fusion/src/model_factories/KerasModelFactory.py\", line 80, in train_model\n      history = model.fit(train_generator, validation_data=validation_generator,\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 569, in safe_patch_function\n      patch_function.call(call_original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 166, in call\n      return cls().__call__(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 170, in __call__\n      return self._patch_implementation(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 228, in _patch_implementation\n      result = super()._patch_implementation(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1302, in _patch_implementation\n      history = original(inst, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n      return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n      original_fn_result = original_fn(*og_args, **og_kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n      original_result = original(*_og_args, **_og_kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/layers/core/dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'sequential_1/dense_2/BiasAdd'\nMatrix size-incompatible: In[0]: [50,2], In[1]: [5,5]\n\t [[{{node sequential_1/dense_2/BiasAdd}}]] [Op:__inference_train_function_4114]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequential\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     33\u001b[0m        model_factory\u001b[38;5;241m.\u001b[39msummary(model)\n\u001b[0;32m---> 34\u001b[0m    history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#       history2 = model_factory.train_model(model, train_generator, validate_generator, model_config)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/dev/AGB/mpf-model-factories/MultiPathFusion/multi_path_fusion/src/model_factories/KerasModelFactory.py:80\u001b[0m, in \u001b[0;36mKerasModelFactory.train_model\u001b[0;34m(self, model, train_generator, validation_generator, model_config)\u001b[0m\n\u001b[1;32m     78\u001b[0m         callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_callbacks(model_config)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#        callbacks = self.create_callbacks(model_config.get(\"callbacks\", {}))\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:569\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m try_log_autologging_event(\n\u001b[1;32m    560\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_start,\n\u001b[1;32m    561\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     kwargs,\n\u001b[1;32m    566\u001b[0m )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_is_class:\n\u001b[0;32m--> 569\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m     patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:166\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mcls\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:177\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_exception(e)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:228\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mactive_run():\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[0;32m--> 228\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run:\n\u001b[1;32m    231\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run(RunStatus\u001b[38;5;241m.\u001b[39mto_string(RunStatus\u001b[38;5;241m.\u001b[39mFINISHED))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py:1302\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1296\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to log training dataset information to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1299\u001b[0m             e,\n\u001b[1;32m   1300\u001b[0m         )\n\u001b[0;32m-> 1302\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[1;32m   1305\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:552\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:487\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    480\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    481\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         og_kwargs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    490\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    491\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         og_kwargs,\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:549\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    546\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m ):\n\u001b[0;32m--> 549\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/dense_2/BiasAdd' defined at (most recent call last):\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3216345/2124766832.py\", line 34, in <module>\n      history = model_factory.train_model(model, train_generator, validate_generator, model_config)\n    File \"/explore/nobackup/people/gtamkin/dev/AGB/mpf-model-factories/MultiPathFusion/multi_path_fusion/src/model_factories/KerasModelFactory.py\", line 80, in train_model\n      history = model.fit(train_generator, validation_data=validation_generator,\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 569, in safe_patch_function\n      patch_function.call(call_original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 166, in call\n      return cls().__call__(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 170, in __call__\n      return self._patch_implementation(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 228, in _patch_implementation\n      result = super()._patch_implementation(original, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1302, in _patch_implementation\n      history = original(inst, *args, **kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 552, in call_original\n      return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 487, in call_original_fn_with_event_logging\n      original_fn_result = original_fn(*og_args, **og_kwargs)\n    File \"/home/gtamkin/.local/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\", line 549, in _original_fn\n      original_result = original(*_og_args, **_og_kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/layers/core/dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'sequential_1/dense_2/BiasAdd'\nMatrix size-incompatible: In[0]: [50,2], In[1]: [5,5]\n\t [[{{node sequential_1/dense_2/BiasAdd}}]] [Op:__inference_train_function_4114]"
     ]
    }
   ],
   "source": [
    "    import time  \n",
    "    \n",
    "    model_type = model_config.get(\"model_type\", \"Sequential\")  \n",
    "    model_factory = get_model_factory(model_type)  \n",
    "    model = model_factory.create_model(model_config, data_generator_config)\n",
    "    model = model_factory.compile_model(model, model_config)\n",
    "\n",
    "    # TODO: potentially add an option for being able to load in a previous model \n",
    "    # (such as using an encoder previously and training on the latent space instead of the original data)\n",
    "\n",
    "    # train_generator = load_data_generator(data_generator_config, 'train')\n",
    "    # validate_generator = load_data_generator(data_generator_config, 'validate')\n",
    "    # test_generator = load_data_generator(data_generator_config, 'test')\n",
    "\n",
    "    # probably not needed - will delete soon\n",
    "    # run_name = get_unique_run_name(mlflow_config, model_config.get('model_name', None))\n",
    "    with mlflow.start_run(experiment_id=mlflow_config['EXPERIMENT_ID'], run_name=model_config.get(\"model_name\", None), description=model_config.get(\"model_description\", \"\"), nested=False) as run:\n",
    "        model_factory.autolog()\n",
    "        log_params(model_config)\n",
    "        RUN_ID = mlflow.active_run().info.run_id\n",
    "        print(f\"STARTING RUN: {RUN_ID}\")\n",
    "        \n",
    "        # TODO: figure out best spot to put this function in, or if there needs to be multiple\n",
    "        # \"extras\" depending on where in the training flow you want it to occur\n",
    "        # put any miscellaneous project-specific code here\n",
    "        model_factory.extras(model, train_generator, validate_generator, test_generator)\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # TODO: figure out a way to make this if statement more generic \n",
    "        # problem is keras model summary best done before training, xgboost summary only able to be done after\n",
    "        if model_type == \"Sequential\" or \"Keras\":\n",
    "            model_factory.summary(model)\n",
    "        history = model_factory.train_model(model, train_generator, validate_generator, model_config)\n",
    " #       history2 = model_factory.train_model(model, train_generator, validate_generator, model_config)\n",
    "        print(f'history: {history.history}')\n",
    " #       print(f'history2: {history2.history}')\n",
    "\n",
    "        t1 = time.time()\n",
    "        total_time = t1 - t0\n",
    "\n",
    "        mlflow.log_param(\"Training time\", total_time)\n",
    "\n",
    "        print()\n",
    "        print(\"Total time for model.fit() processing is: \" + str(total_time) + \" seconds.\")\n",
    "\n",
    "        if model_type == \"XGBoost\":\n",
    "            model_factory.summary(model)\n",
    "\n",
    "        predictions = model_factory.predict(model, test_generator)\n",
    "        test_results = model_factory.evaluate(model, test_generator)\n",
    "        \n",
    "        # debugging\n",
    "        print(f'predictions: {predictions}')\n",
    "        print(f'test results: {test_results}')\n",
    "\n",
    "        model_factory.log_metrics(test_results)\n",
    "        model_factory.plot_metrics(model, test_generator, history)\n",
    "\n",
    "        \n",
    "        # debugging\n",
    "        print(\"Finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices\n"
     ]
    }
   ],
   "source": [
    "branch_num = file_num = 0\n",
    "full_tile = data_generator_config['branch_inputs'][branch_num][\"branch_files\"][file_num][\"mlbs_year_filepath\"]\n",
    "full_tile_prefix = os.path.splitext(full_tile)  # returns ('/home/user/somefile', '.txt')\n",
    "tile_prefix = str(full_tile_prefix[0]) \n",
    "print(tile_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices/MLP_7bands_2_epochs::138094747525594611/\n"
     ]
    }
   ],
   "source": [
    "archive_id = mlflow_config['TRACKING_URI'] +  \"/\" + tile_prefix + \"/\" + model_config.get(\"model_name\") + '::' + mlflow_config['EXPERIMENT_ID'] + '/'\n",
    "if not os.path.exists(archive_id):\n",
    "    os.makedirs(archive_id)\n",
    "print(archive_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Computer Name is:gpu004\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m archive_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# another_strategy = tf.distribute.MirroredStrategy()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# with another_strategy.scope():\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     load_options = tf.saved_model.LoadOptions(experimental_io_device=hostname)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# loaded = tf.keras.models.load_model(archive_model_id, options=load_options)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_model_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:961\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    958\u001b[0m   loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    959\u001b[0m                   ckpt_options, options, filters)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 961\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    962\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    966\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    967\u001b[0m root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "IPAddr = socket.gethostbyname(hostname)\n",
    "print(\"Your Computer Name is:\" + hostname)\n",
    "\n",
    "archive_model_id = '/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model'\n",
    "# another_strategy = tf.distribute.MirroredStrategy()\n",
    "# with another_strategy.scope():\n",
    "#     load_options = tf.saved_model.LoadOptions(experimental_io_device=hostname)\n",
    "#     loaded = tf.keras.models.load_model(archive_model_id, options=load_options)\n",
    "\n",
    "# load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "# loaded = tf.keras.models.load_model(archive_model_id, options=load_options)\n",
    "\n",
    "loaded = tf.keras.models.load_model(archive_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/restore model() with tensorflow.keras.models() and pickle().  Gets errors with both approaches (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/assets\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m archive_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave(archive_model_id)\n\u001b[0;32m----> 3\u001b[0m model_rk \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(archive_model_id, model, model_rk)\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:961\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    958\u001b[0m   loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    959\u001b[0m                   ckpt_options, options, filters)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 961\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    962\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    966\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    967\u001b[0m root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "archive_model_id = '/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/20240115/600584965588330414/MODELS/MLP_7bands_2_epochs::600584965588330414.keras[[2, 16]].model'\n",
    "model.save(archive_model_id)\n",
    "model_rk = tf.keras.models.load_model(archive_model_id)\n",
    "print(archive_model_id, model, model_rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices/MLP_7bands_2_epochs::138094747525594611/MLP_7bands_2_epochs.keras_k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices/MLP_7bands_2_epochs::138094747525594611/MLP_7bands_2_epochs.keras_k/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/projects/ilab/data/AGB/test/mlruns/exp_7bands/MLBS_2018_541567.6_4136443.0_542567.6_4137443.0/MLBS_2018_hyperspectral_indices/MLP_7bands_2_epochs::138094747525594611/MLP_7bands_2_epochs.keras_k <keras.engine.sequential.Sequential object at 0x1460c88c9af0> <keras.engine.sequential.Sequential object at 0x145ff5de8f10>\n"
     ]
    }
   ],
   "source": [
    "archive_model_id = archive_id + model_config.get(\"model_name\") + \".keras_k\"\n",
    "model.save(archive_model_id)\n",
    "model_rk = tf.keras.models.load_model(archive_model_id)\n",
    "print(archive_model_id, model, model_rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "# differences = DeepDiff(model, model_rk)\n",
    "# pprint(model, differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2024-01-15 10:14:58           64\n",
      "config.json                                    2024-01-15 10:14:58         1372\n",
      "variables.h5                                   2024-01-15 10:14:58        21392\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "metadata.json                                  2024-01-15 10:14:58           64\n",
      "config.json                                    2024-01-15 10:14:58         1372\n",
      "variables.h5                                   2024-01-15 10:14:58        21392\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "archive_model_id = archive_id + model_config.get(\"model_name\") + \".keras_p\"\n",
    "pickle.dump(model, open(archive_model_id, \"wb\"))\n",
    "model_rp = pickle.load(open(archive_model_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model I/O: \n",
      "\n",
      "model -> trained model: \n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[-0.04904908, -0.73021334,  0.08650863, -0.49822754, -0.69361174],\n",
      "       [ 0.7660165 , -0.17820251, -0.26483762,  0.42923212, -0.3845378 ],\n",
      "       [-0.25372738, -0.51191515,  0.21147764, -0.6425986 , -0.56746954],\n",
      "       [ 0.26987362, -0.57026625,  0.4126774 , -0.27614143, -0.7255419 ],\n",
      "       [ 0.25262725, -0.38714802,  0.06436145, -0.22253472, -0.02028376]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[-0.27380675, -0.64187217, -0.30194208, -0.5833962 , -0.49994192],\n",
      "       [-0.26948434,  0.19278693,  0.6494565 ,  0.6940801 , -0.2629711 ],\n",
      "       [ 0.7647644 , -0.21074575, -0.11840165, -0.63423127,  0.48843348],\n",
      "       [ 0.04930228,  0.7143147 ,  0.50581765, -0.5671184 ,  0.56588864],\n",
      "       [-0.63720566,  0.0790351 , -0.60316193, -0.67758685, -0.3336277 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]\n",
      "\n",
      "model_r -> trained model: \n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[-0.04904908, -0.73021334,  0.08650863, -0.49822754, -0.69361174],\n",
      "       [ 0.7660165 , -0.17820251, -0.26483762,  0.42923212, -0.3845378 ],\n",
      "       [-0.25372738, -0.51191515,  0.21147764, -0.6425986 , -0.56746954],\n",
      "       [ 0.26987362, -0.57026625,  0.4126774 , -0.27614143, -0.7255419 ],\n",
      "       [ 0.25262725, -0.38714802,  0.06436145, -0.22253472, -0.02028376]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(5, 5) dtype=float32, numpy=\n",
      "array([[-0.27380675, -0.64187217, -0.30194208, -0.5833962 , -0.49994192],\n",
      "       [-0.26948434,  0.19278693,  0.6494565 ,  0.6940801 , -0.2629711 ],\n",
      "       [ 0.7647644 , -0.21074575, -0.11840165, -0.63423127,  0.48843348],\n",
      "       [ 0.04930228,  0.7143147 ,  0.50581765, -0.5671184 ,  0.56588864],\n",
      "       [-0.63720566,  0.0790351 , -0.60316193, -0.67758685, -0.3336277 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print('Model I/O: ')\n",
    "print('\\nmodel -> trained model: ')\n",
    "print(model.weights)\n",
    "\n",
    "print('\\nmodel_r -> trained model: ')\n",
    "print(model_rk.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a scalar tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m differences \u001b[38;5;241m=\u001b[39m \u001b[43mDeepDiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_rk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pprint(model, differences)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:320\u001b[0m, in \u001b[0;36mDeepDiff.__init__\u001b[0;34m(self, t1, t2, cache_purge_level, cache_size, cache_tuning_sample_size, custom_operators, cutoff_distance_for_pairs, cutoff_intersection_for_pairs, encodings, exclude_obj_callback, exclude_obj_callback_strict, exclude_paths, include_obj_callback, include_obj_callback_strict, include_paths, exclude_regex_paths, exclude_types, get_deep_distance, group_by, group_by_sort_key, hasher, hashes, ignore_encoding_errors, ignore_nan_inequality, ignore_numeric_type_changes, ignore_order, ignore_order_func, ignore_private_variables, ignore_string_case, ignore_string_type_changes, ignore_type_in_groups, ignore_type_subclasses, iterable_compare_func, zip_ordered_iterables, log_frequency_in_sec, math_epsilon, max_diffs, max_passes, number_format_notation, number_to_string_func, progress_logger, report_repetition, significant_digits, truncate_datetime, verbose_level, view, _original_type, _parameters, _shared_parameters, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m root \u001b[38;5;241m=\u001b[39m DiffLevel(t1, t2, verbose_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_level)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# _original_type is only used to pass the original type of the data. Currently only used for numpy arrays.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# The reason is that we convert the numpy array to python list and then later for distance calculations\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# we convert only the the last dimension of it into numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfrozenset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_deep_distance \u001b[38;5;129;01mand\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m {TEXT_VIEW, TREE_VIEW}:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeep_distance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rough_distance()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:1577\u001b[0m, in \u001b[0;36mDeepDiff._diff\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_obj(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Iterable):\n\u001b[0;32m-> 1577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Enum):\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_enum(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:663\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_iterable_with_deephash(level, parents_ids, _original_type\u001b[38;5;241m=\u001b[39m_original_type, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable_in_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:792\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable_in_order\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[report_type] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m levels\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_by_forming_pairs_and_comparing_one_by_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_relationship_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_relationship_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:895\u001b[0m, in \u001b[0;36mDeepDiff._diff_by_forming_pairs_and_comparing_one_by_one\u001b[0;34m(self, level, local_tree, parents_ids, _original_type, child_relationship_class, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Go one level deeper\u001b[39;00m\n\u001b[1;32m    889\u001b[0m next_level \u001b[38;5;241m=\u001b[39m level\u001b[38;5;241m.\u001b[39mbranch_deeper(\n\u001b[1;32m    890\u001b[0m     x,\n\u001b[1;32m    891\u001b[0m     y,\n\u001b[1;32m    892\u001b[0m     child_relationship_class\u001b[38;5;241m=\u001b[39mchild_relationship_class,\n\u001b[1;32m    893\u001b[0m     child_relationship_param\u001b[38;5;241m=\u001b[39mj\n\u001b[1;32m    894\u001b[0m )\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids_added\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:1577\u001b[0m, in \u001b[0;36mDeepDiff._diff\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_obj(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Iterable):\n\u001b[0;32m-> 1577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Enum):\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_enum(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:663\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_iterable_with_deephash(level, parents_ids, _original_type\u001b[38;5;241m=\u001b[39m_original_type, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable_in_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:792\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable_in_order\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[report_type] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m levels\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_by_forming_pairs_and_comparing_one_by_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_relationship_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_relationship_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:895\u001b[0m, in \u001b[0;36mDeepDiff._diff_by_forming_pairs_and_comparing_one_by_one\u001b[0;34m(self, level, local_tree, parents_ids, _original_type, child_relationship_class, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Go one level deeper\u001b[39;00m\n\u001b[1;32m    889\u001b[0m next_level \u001b[38;5;241m=\u001b[39m level\u001b[38;5;241m.\u001b[39mbranch_deeper(\n\u001b[1;32m    890\u001b[0m     x,\n\u001b[1;32m    891\u001b[0m     y,\n\u001b[1;32m    892\u001b[0m     child_relationship_class\u001b[38;5;241m=\u001b[39mchild_relationship_class,\n\u001b[1;32m    893\u001b[0m     child_relationship_param\u001b[38;5;241m=\u001b[39mj\n\u001b[1;32m    894\u001b[0m )\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids_added\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:1577\u001b[0m, in \u001b[0;36mDeepDiff._diff\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_obj(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Iterable):\n\u001b[0;32m-> 1577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Enum):\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_enum(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:663\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_iterable_with_deephash(level, parents_ids, _original_type\u001b[38;5;241m=\u001b[39m_original_type, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable_in_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:792\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable_in_order\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[report_type] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m levels\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_by_forming_pairs_and_comparing_one_by_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_relationship_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_relationship_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:895\u001b[0m, in \u001b[0;36mDeepDiff._diff_by_forming_pairs_and_comparing_one_by_one\u001b[0;34m(self, level, local_tree, parents_ids, _original_type, child_relationship_class, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Go one level deeper\u001b[39;00m\n\u001b[1;32m    889\u001b[0m next_level \u001b[38;5;241m=\u001b[39m level\u001b[38;5;241m.\u001b[39mbranch_deeper(\n\u001b[1;32m    890\u001b[0m     x,\n\u001b[1;32m    891\u001b[0m     y,\n\u001b[1;32m    892\u001b[0m     child_relationship_class\u001b[38;5;241m=\u001b[39mchild_relationship_class,\n\u001b[1;32m    893\u001b[0m     child_relationship_param\u001b[38;5;241m=\u001b[39mj\n\u001b[1;32m    894\u001b[0m )\n\u001b[0;32m--> 895\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids_added\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:1577\u001b[0m, in \u001b[0;36mDeepDiff._diff\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_obj(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Iterable):\n\u001b[0;32m-> 1577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(level\u001b[38;5;241m.\u001b[39mt1, Enum):\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_enum(level, parents_ids, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:663\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diff_iterable_with_deephash(level, parents_ids, _original_type\u001b[38;5;241m=\u001b[39m_original_type, local_tree\u001b[38;5;241m=\u001b[39mlocal_tree)\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_iterable_in_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:792\u001b[0m, in \u001b[0;36mDeepDiff._diff_iterable_in_order\u001b[0;34m(self, level, parents_ids, _original_type, local_tree)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[report_type] \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m levels\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diff_by_forming_pairs_and_comparing_one_by_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparents_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_relationship_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_relationship_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:821\u001b[0m, in \u001b[0;36mDeepDiff._diff_by_forming_pairs_and_comparing_one_by_one\u001b[0;34m(self, level, local_tree, parents_ids, _original_type, child_relationship_class, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_diff_by_forming_pairs_and_comparing_one_by_one\u001b[39m(\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mself\u001b[39m, level, local_tree, parents_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfrozenset\u001b[39m(),\n\u001b[1;32m    816\u001b[0m     _original_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, child_relationship_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    817\u001b[0m     t1_from_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, t1_to_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    818\u001b[0m     t2_from_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, t2_to_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    819\u001b[0m ):\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (i, j), (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_matching_pairs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt1_from_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1_from_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1_to_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1_to_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt2_from_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt2_from_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2_to_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt2_to_index\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_diff() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# pragma: no cover. This is already covered for addition.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:702\u001b[0m, in \u001b[0;36mDeepDiff._get_matching_pairs\u001b[0;34m(self, level, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03mGiven a level get matching pairs. This returns list of two tuples in the form:\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m[\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03mDefault it to compare in order\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_compare_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# Match in order if there is no compare function provided\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compare_in_order\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt1_from_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1_from_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1_to_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1_to_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt2_from_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt2_from_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2_to_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt2_to_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m     matches \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/deepdiff/diff.py:676\u001b[0m, in \u001b[0;36mDeepDiff._compare_in_order\u001b[0;34m(self, level, t1_from_index, t1_to_index, t2_from_index, t2_to_index)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03mDefault compare if `iterable_compare_func` is not provided.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03mThis will compare in sequence order.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t1_from_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [((i, i), (x, y)) \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m--> 676\u001b[0m         \u001b[43mzip_longest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mListItemRemovedOrAdded\u001b[49m\u001b[43m)\u001b[49m)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     t1_chunk \u001b[38;5;241m=\u001b[39m level\u001b[38;5;241m.\u001b[39mt1[t1_from_index:t1_to_index]\n",
      "File \u001b[0;32m/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:583\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a tensor with unknown shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape:\n\u001b[0;32m--> 583\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a scalar tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    586\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot iterate over a tensor with unknown first dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot iterate over a scalar tensor."
     ]
    }
   ],
   "source": [
    "# differences = DeepDiff(model.weights, model_rk.weights)\n",
    "# pprint(model, differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain each restored model and compare history.  The histories were saved cleanly.\n",
    "### Histories from restored models seem to reflect that they were saved sufficiently despite tracebacks about certain attributes.  But use the Keras() to save instead of pickle to be safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        history_rk = model_factory.train_model(model_rk, train_generator, validate_generator, model_config)\n",
    "        history_rp = model_factory.train_model(model_rp, train_generator, validate_generator, model_config)\n",
    "        print(f'history_rk: {history_rk.history}')\n",
    "        print(f'history_rp: {history_rp.history}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences = DeepDiff(history.history, history2.history)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences = DeepDiff(history_rk.history, history_rp.history)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore each trained model history and compare results using pickle().  The history was saved cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archive_history_id = archive_id + model_config.get(\"model_name\") + \"_history.fit\"\n",
    "pickle.dump(history, open(archive_history_id, \"wb\"))\n",
    "history_r = pickle.load(open(archive_history_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = DeepDiff(history.history, history_r.history)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore test_results() from each evaluation using pickle(). The test_results() were saved cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_evaluate_id = archive_id + model_config.get(\"model_name\") + \"_evaluate.results\"\n",
    "pickle.dump(test_results, open(archive_evaluate_id, \"wb\"))\n",
    "test_results_r = pickle.load(open(archive_evaluate_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = DeepDiff(test_results, test_results_r)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('History I/O: ')\n",
    "print('\\nhistory -> trained model history: ')\n",
    "print('epoch', history.epoch)\n",
    "print('history', history.history)\n",
    "print('model.weights', history.model.weights)\n",
    "print('params', history.params)\n",
    "\n",
    "# print('\\nhistory_r -> trained model history: ')\n",
    "# print('epoch', history_r.epoch)\n",
    "# print('history', history_r.history)\n",
    "# print('model.weights', history_r.model.weights)\n",
    "# print('params', history_r.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluate I/O: ')\n",
    "print('\\nevaluate -> test_results: ')\n",
    "print('test loss', test_results['test loss'])\n",
    "print('test accuracy', test_results['test accuracy'])\n",
    "\n",
    "# print('\\nevaluate_r -> test_results: ')\n",
    "# print('test loss', test_results_r['test loss'])\n",
    "# print('test accuracy', test_results_r['test accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore test_generator() data using pickle. The test_generator() was saved cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_test_generator_id = archive_id + model_config.get(\"model_name\") + \"_test_generator.data\"\n",
    "pickle.dump(test_generator, open(archive_test_generator_id, \"wb\"))\n",
    "test_generator_r = pickle.load(open(archive_test_generator_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = DeepDiff(test_generator, test_generator_r)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Test Generator I/O: ')\n",
    "print('\\ntest_generator -> data: ')\n",
    "print('batch_size', test_generator.batch_size)\n",
    "print('binned', test_generator.binned)\n",
    "print('len(binned)', len(test_generator.binned))\n",
    "print('branch_inputs', test_generator.branch_inputs)\n",
    "print('file_x_stack.shape', test_generator.file_x_stack.shape)\n",
    "print('file_x_stack', test_generator.file_x_stack)\n",
    "print('file_x_stack.min()', test_generator.file_x_stack.min())\n",
    "print('file_x_stack.max()', test_generator.file_x_stack.max())\n",
    "print('file_x_stack.mean()', test_generator.file_x_stack.mean())\n",
    "print('file_x_stack[6].mean()', test_generator.file_x_stack[6].mean())\n",
    "print('truthset_a', test_generator.truthset_a)\n",
    "\n",
    "print('\\ntest_generator_r -> data: ')\n",
    "# print('batch_size', test_generator_r.batch_size)\n",
    "# print('binned', test_generator_r.binned)\n",
    "# print('len(binned)', len(test_generator_r.binned))\n",
    "# print('branch_inputs', test_generator_r.branch_inputs)\n",
    "# print('file_x_stack.shape', test_generator_r.file_x_stack.shape)\n",
    "# print('file_x_stack', test_generator_r.file_x_stack)\n",
    "# print('file_x_stack.min()', test_generator_r.file_x_stack.min())\n",
    "# print('file_x_stack.max()', test_generator_r.file_x_stack.max())\n",
    "# print('file_x_stack.mean()', test_generator_r.file_x_stack.mean())\n",
    "# print('file_x_stack[6].mean()', test_generator_r.file_x_stack[6].mean())\n",
    "print('truthset_a', test_generator_r.truthset_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/19/2023 - ONTO SHAP VALUES NOW THAT WE'VE CREATED, TRAINED (fit), EVALUATED AND STORED THE MODEL, MODEL HISTORY and TEST-GENERATOR DATA.  \n",
    "### NOTE: PREDICTIONS NOT IMPORTANT UNTIL PERMUATION IMPORTANCE IS IDENTIFIED (ala MONTE CARLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "keywordAll = ['ARI', 'CAI', 'CRI550', 'CRI700', 'EVI', 'EVI2', 'fPAR', 'LAI', 'MCTI', 'MSI',\n",
    "                'NDII', 'NDLI', 'NDNI', 'NDVI', 'NDWI', 'NIRv', 'PRIn', 'PRIw', 'SAVI', 'WBI', 'Albedo']\n",
    "keywordAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_feature_importances_shap_values(shap_values, features):\n",
    "    \n",
    "    '''\n",
    "    Prints the feature importances based on SHAP values in an ordered way\n",
    "    shap_values -> The SHAP values calculated from a shap.Explainer object\n",
    "    features -> The name of the features, on the order presented to the explainer\n",
    "    '''\n",
    "\n",
    "    # Calculates the feature importance (mean absolute shap value) for each feature\n",
    "    importances = []\n",
    "    for i in range(shap_values.values.shape[1]):\n",
    "        importances.append(np.mean(np.abs(shap_values.values[:, i])))\n",
    "        \n",
    "    # Calculates the normalized version\n",
    "    importances_norm = softmax(importances)\n",
    "\n",
    "    # Organize the importances and columns in a dictionary\n",
    "    feature_importances = {fea: imp for imp, fea in zip(importances, features)}\n",
    "    feature_importances_norm = {fea: imp for imp, fea in zip(importances_norm, features)}\n",
    "\n",
    "    # Sorts the dictionary\n",
    "    feature_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1], reverse = True)}\n",
    "    feature_importances_norm= {k: v for k, v in sorted(feature_importances_norm.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "    # Prints the feature importances\n",
    "    for k, v in feature_importances.items():\n",
    "        print(f\"{k} -> {v:.4f} (softmax = {feature_importances_norm[k]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_regression(y, y_pred):\n",
    "    \n",
    "    '''\n",
    "    Prints the most common evaluation metrics for regression\n",
    "    '''\n",
    "    \n",
    "    mae = MAE(y, y_pred)\n",
    "    mse = MSE(y, y_pred)\n",
    "    rmse = mse ** (1/2)\n",
    "    r2 = R2(y, y_pred)\n",
    "    \n",
    "    print('Regression result')\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test_generator.file_x_stack to Pandas dataframe and transpose for SHAP API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=test_generator_r.file_x_stack)\n",
    "dft = df.transpose()\n",
    "X = X_test = dft\n",
    "print(df.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _get_shap_values(self, model, test_generator, X):\n",
    "\n",
    "\n",
    "\n",
    "#         import shap\n",
    "\n",
    "#         explainer = shap.KernelExplainer(model.predict, X.iloc[:50, :])\n",
    "\n",
    "#         self.mpfConfig.shap_values0to50 = self.mpfConfig.explainer.shap_values(X.iloc[0:50, :], nsamples=500)\n",
    "#         # self.mpfConfig.shap_values51to100 = self.mpfConfig.explainer.shap_values(X.iloc[50:99, :], nsamples=500)\n",
    "#         # self.mpfConfig.shap_value101to150 = self.mpfConfig.explainer.shap_values(X.iloc[100:149, :], nsamples=500)\n",
    "#         # self.mpfConfig.shap_values50 = self.mpfConfig.explainer.shap_values(X.iloc[280:330, :], nsamples=500)\n",
    "\n",
    "#         shap.summary_plot(self.mpfConfig.shap_values0to50[0], X.iloc[0:50, :], plot_type=\"bar\", feature_names=keyword)\n",
    "#         shap.summary_plot(shap_values0to50[0], X.iloc[280:330, :], plot_type=\"bar\", feature_names=keyword)\n",
    "#         print(len(self.mpfConfig.shap_values))\n",
    "#         print(self.mpfConfig.shap_values)\n",
    "#         return self.mpfConfig.explainer, self.mpfConfig.shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use Explainer to explain the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model\n",
    "y_pred = model.predict(X_test)\n",
    "#evaluate_regression(X_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fits the explainer\n",
    "explainer = shap.Explainer(model.predict, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculates the SHAP values - It takes some time\n",
    "shap_values = explainer(X_test.iloc[:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Use KernelExplainer to explain the output of any function. https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html\n",
    "\n",
    "## Kernel SHAP is a method that uses a special weighted linear regression to compute the importance of each feature. The computed importance values are Shapley values from game theory and also coefficents from a local linear regression.\n",
    "\n",
    "## Parameters:\n",
    "### model [function or iml.Model]:\n",
    "### User supplied function that takes a matrix of samples (# samples x # features) and computes a the output of the model for those samples. The output can be a vector (# samples) or a matrix (# samples x # model outputs).\n",
    "\n",
    "### data [datanumpy.array or pandas.DataFrame or shap.common.DenseData or any scipy.sparse matrix]:\n",
    "### The background dataset to use for integrating out features. To determine the impact of a feature, that feature is set to “missing” and the change in the model output is observed. Since most models aren’t designed to handle arbitrary missing data at test time, we simulate “missing” by replacing the feature with the values it takes in the background dataset. So if the background dataset is a simple sample of all zeros, then we would approximate a feature being missing by setting it to zero. For small problems this background dataset can be the whole training set, but for larger problems consider using a single reference value or using the kmeans function to summarize the dataset. Note: for sparse case we accept any sparse matrix but convert to lil format for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above.  In this call, we use the first 50 rows of the test-generator dataset as the background dataset.  Should be configurable.  QUESTION: Should this be train_generator instead (perhaps with K-means)\n",
    "explainer = shap.KernelExplainer(model.predict, X.iloc[:50, :])\n",
    "print(explainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/restore KernelExplainer data using pickle().  Explainer saved cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_explainer_id = archive_id + model_config.get(\"model_name\") + \"_kernel.explainer\"\n",
    "pickle.dump(explainer, open(archive_explainer_id, \"wb\"))\n",
    "explainer_r = pickle.load(open(archive_explainer_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = DeepDiff(explainer, explainer_r)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KernelExplainer I/O: ')\n",
    "print('\\n Explainer -> features: ')\n",
    "print('data_feature_names', explainer.data_feature_names)\n",
    "print('len(expected_value)', len(explainer.expected_value))\n",
    "print('expected_value', explainer.expected_value)\n",
    "\n",
    "print('\\n Explainer_r -> features: ')\n",
    "print('data_feature_names', explainer_r.data_feature_names)\n",
    "print('len(expected_value)', len(explainer_r.expected_value))\n",
    "print('expected_value', explainer_r.expected_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_explainer_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the SHAP values for a set of samples:  shap_values(X, **kwargs)\n",
    "## https://shap.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#shap.KernelExplainer.shap_values\n",
    "\n",
    "## Parameters:\n",
    "## Xnumpy.array or pandas.DataFrame or any scipy.sparse matrix\n",
    "### A matrix of samples (# samples x # features) on which to explain the model’s output.\n",
    "## nsamples“auto” or int\n",
    "### Number of times to re-evaluate the model when explaining each prediction. More samples lead to lower variance estimates of the SHAP values. The “auto” setting uses nsamples = 2 * X.shape[1] + 2048.\n",
    "## gc_collectbool\n",
    "### Run garbage collection after each explanation round. Sometime needed for memory intensive explanations (default False).\n",
    "## Returns:  array or list\n",
    "### For models with a single output this returns a matrix of SHAP values (# samples x # features). Each row sums to the difference between the model output for that sample and the expected value of the model output (which is stored as ## expected_value attribute of the explainer). For models with vector outputs this returns a list of such matrices, one for each output.  \n",
    "\n",
    "# *NOTE:  In our nomenclature, features = bands = indices*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shap values for the first 50 rows in the test-generator dataset - based on 500 samples each (should be configurable)\n",
    "shap_values0to50 = explainer.shap_values(X.iloc[0:50, :], nsamples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sshap_values0to50.shape() = (50, 7). - first 50 rows by 7 band columns\n",
    "# there are 5 rows of shape values because binning results in 5 classifications\n",
    "#print(len(shap_values0to50), shap_values0to50[0].shape , shap_values0to50[4][0].shape)\n",
    "lastShapRow = shap_values0to50[0][0]\n",
    "print(lastShapRow.shape, lastShapRow[0].max(), lastShapRow)\n",
    "sum(explainer_r.expected_value[0] - lastShapRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandLen = 20\n",
    "\n",
    "bandOccurenceArr = np.zeros(bandLen)\n",
    "print(bandOccurenceArr)\n",
    "bandMaxArr = np.zeros(bandLen)\n",
    "bandMinArr = np.zeros(bandLen)\n",
    "bandMeanArr = np.zeros(bandLen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#shap_values0to50 = explainer.shap_values(X.iloc[0:50, :], nsamples=500)\n",
    "shap_values_explanation = explainer(X.iloc[0:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(shap_values_explanation[0], shap_values_explanation[0].values[0][0],  shap_values_explanation[0].data[0])\n",
    "print(shap_values_explanation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_delta = (shap_values_explanation.base_values[0] - shap_values_explanation.values[0])\n",
    "pprint(base_delta)\n",
    "col_totals = [ sum(x) for x in zip(*base_delta) ]\n",
    "pprint(col_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('expected_value', explainer_r.expected_value)\n",
    "print(' - expected_value', explainer_r.expected_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap_values_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.iloc[0:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_shap_values_explanation_id = archive_id + model_config.get(\"model_name\") + \"_shap.explanation\"\n",
    "pickle.dump(shap_values_explanation, open(archive_shap_values_explanation_id, \"wb\"))\n",
    "archive_shap_values_explanation_id_r = pickle.load(open(archive_shap_values_explanation_id, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = DeepDiff(shap_values_explanation, shap_values_explanation_id_r)\n",
    "pprint(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat logic using X_train instead of parts of X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(data=train_generator.file_x_stack)\n",
    "dft = df.transpose()\n",
    "X_train = dft\n",
    "print(df.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above.  In this call, we use the first 50 rows of the test-generator dataset as the background dataset.  Should be configurable.\n",
    "explainer_X_train = shap.KernelExplainer(model.predict, X_train)\n",
    "#explainer_X_train = shap.KernelExplainer(model.predict, X_train.iloc[:50, :])\n",
    "print(explainer_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values0to50_X_train = explainer.shap_values(X.iloc[0:50, :], nsamples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model  \n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(f):\n",
    "    print(\n",
    "        \"Root mean squared test error = {}\".format(\n",
    "            np.sqrt(np.mean((f(X_test) - y_test) ** 2))\n",
    "        )\n",
    "    )\n",
    "    time.sleep(0.5)  # to let the print get out before any progress bars\n",
    "\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== END: GENERAL Model creation, training, prediction, evaluation from MPF above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== MAIN THREAD FROM 11/25: shap.KernelExplainer\n",
    "## https://jh-ml.nccs.nasa.gov/jupyterhub-prism/user/gtamkin/lab/tree/_AGB-dev/shap/notebooks/tabular_examples/neural_networks/Census%20income%20classification%20with%20Keras.ipynb\n",
    "## https://stackoverflow.com/questions/45361559/feature-importance-chart-in-neural-network-using-keras-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=test_generator.file_x_stack)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.transpose()\n",
    "dft.iloc[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dft\n",
    "print(X.shape)\n",
    "print(X.shape[0])\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[1])\n",
    "count = 0\n",
    "for i in range(X.shape[1]):\n",
    "    count = count + 1\n",
    "\n",
    "print(count)\n",
    "#print(X[:,count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([X[:, i] for i in range(X.shape[1])])\n",
    "print(X.shape)\n",
    "print(X.iloc[:, :])\n",
    "print(X.iloc[:5, :])\n",
    "print(X.iloc[0, 0])\n",
    "print(X.iloc[3, 6])\n",
    "print(X.iloc[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict2(X):\n",
    "    return model.predict([X[:, i] for i in range(X.shape[1])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# load your data here, e.g. X and y\n",
    "# create and fit your model here\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "print(model.summary())\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.KernelExplainer(model.predict, X.iloc[:50, :])\n",
    "#explainer = shap.KernelExplainer(my_predict2, X.iloc[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X.iloc[:50, :])\n",
    "shap_values = explainer.shap_values(X.iloc[299, :], nsamples=500)\n",
    "shap_values2 = explainer.shap_values(X.iloc[299, :], nsamples=500)\n",
    "print('shap_values: '+ str(len(shap_values)))\n",
    "print(shap_values)\n",
    "print('shap_values2: '+ str(len(shap_values2)))\n",
    "print(shap_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[299, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[0].tofile('./mlruns/shap1.shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values0 = np.fromfile('./mlruns/shap1.shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.file_x_stack.shape, train_generator.file_x_stack[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values[0], sum(shap_values[0]))\n",
    "print(shap_values0, sum(shap_values0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explainer.fx, sum(explainer.fx))\n",
    "print(explainer.expected_value, sum(explainer.expected_value))\n",
    "diff = explainer.fx - explainer.expected_value\n",
    "print(diff, sum(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 3\n",
    "bandList = data_generator_config['branch_inputs'][0]['branch_files'][0]['bands']\n",
    "bandList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandRootDir = '/explore/nobackup/projects/ilab/data/AGB/test/mlruns/12122023-7b/BANDS'\n",
    "hyperspectralIndicesFile = data_generator_config['branch_inputs'][0]['branch_files'][0]['mlbs_year_filepath'] \n",
    "print(hyperspectralIndicesFile)\n",
    "hpath, hname = hyperspectralIndicesFile.split('/',1)\n",
    "hprefix, hsuffix = hname.split('.',1)\n",
    "print(bandRootDir, hpath, hprefix)\n",
    "newPath = os.path.join(bandRootDir, hpath)\n",
    "print(newPath)\n",
    "if (not os.path.exists(newPath)): os.makedirs(newPath)\n",
    "\n",
    "# Write out bands\n",
    "for i in range(len(bandList)):\n",
    "    newPathFile = os.path.join(newPath, hprefix+'_band'+str(bandList[i]).zfill(padding)+'.band')\n",
    "    print(i, str(bandList[i]).zfill(padding), newPathFile, train_generator.file_x_stack[i].dtype, train_generator.file_x_stack[i].shape, train_generator.file_x_stack[i].min())\n",
    "    train_generator.file_x_stack[i].tofile(newPathFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in bands\n",
    "for i in range(len(bandList)):\n",
    "    existingBandFile = os.path.join(newPath, hprefix+'_band'+str(bandList[i]).zfill(padding)+'.band')\n",
    "    bandValues = np.fromfile(existingBandFile)\n",
    "    print(existingBandFile, bandValues.dtype, bandValues.shape, bandValues.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.file_x_stack[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.file_x_stack[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile\n",
    "outfile = TemporaryFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band018 = train_generator.file_x_stack[0]\n",
    "band014 = train_generator.file_x_stack[1]\n",
    "band009 = train_generator.file_x_stack[2]\n",
    "band003 = train_generator.file_x_stack[3]\n",
    "band020 = train_generator.file_x_stack[4]\n",
    "band008 = train_generator.file_x_stack[5]\n",
    "band010 = train_generator.file_x_stack[6]\n",
    "print(band018, band018.dtype, band018.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(outfile, band018=band018, band014=band014, band009=band009, band003=band003, band020=band020, band008=band008, band010=band010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = outfile.seek(0)\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(band018, band018.dtype, band018.shape, band018.max())\n",
    "print(npzfile['band018'], npzfile['band018'].dtype, npzfile['band018'].shape, npzfile['band018'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile\n",
    "band018file = TemporaryFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(band018file, train_generator.file_x_stack[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = band018file.seek(0)\n",
    "b18 = np.load(band018file)\n",
    "print(b18, b18.dtype, b18.shape, b18.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1    Index Name=ARI\n",
    "#2    Index Name=CAI\n",
    "#3    Index Name=CRI550\n",
    "#4    Index Name=CRI700\n",
    "#5    Index Name=EVI\n",
    "#6    Index Name=EVI2\n",
    "#7    Index Name=fPAR\n",
    "#8    Index Name=LAI\n",
    "#9    Index Name=MCTI\n",
    "#10    Index Name=MSI\n",
    "#11   Index Name=NDII\n",
    "#12    Index Name=NDLI\n",
    "#13    Index Name=NDNI\n",
    "#14    Index Name=NDVI\n",
    "#15    Index Name=NDWI\n",
    "#16    Index Name=NIRv\n",
    "#17    Index Name=PRIn\n",
    "#18    Index Name=PRIw\n",
    "#19    Index Name=SAVI\n",
    "#20    Index Name=WBI\n",
    "#21    Index Name=Albedo\n",
    "\n",
    "#        \"bands\": [18, 14, 9, 3, 20, 8, 10]}]\n",
    "    \n",
    "keyword = list()\n",
    "keyword.append('PRIw')\n",
    "keyword.append('NDVI')\n",
    "keyword.append('MCTI')\n",
    "keyword.append('CRI550')\n",
    "keyword.append('WBI')\n",
    "keyword.append('LAI')\n",
    "keyword.append('MSI')\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "while x < 5:\n",
    "    diff = explainer.expected_value[x] - shap_values[x]\n",
    "    print(\"explainer.expected_value[x] where x = \" + str(x))\n",
    "    print(explainer.expected_value[x], shap_values[x], diff, \n",
    "      diff.min(), diff.max())\n",
    "    x = x + 1\n",
    "print(x)\n",
    "print(explainer.expected_value, sum(explainer.expected_value))\n",
    "print(explainer.fx, sum(explainer.fx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "#X_test_kmeans_summary = shap.kmeans(X, 1000)\n",
    "#print(X_test_kmeans_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test_kmeans_summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shap_values: ' + str(len(shap_values)), shap_values)\n",
    "print('shap_values2: '+ str(len(shap_values2)), shap_values2)\n",
    "shap_values999 = explainer.shap_values(X.iloc[999, :], nsamples=500)\n",
    "print('shap_values999: '+ str(len(shap_values999)), shap_values999)\n",
    "shap_values999b = explainer.shap_values(X.iloc[999, :], nsamples=500)\n",
    "print('shap_values999b: '+ str(len(shap_values999b)), shap_values999b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot(shap_values, X.iloc[0, :], plot_type=\"bar\", feature_names=keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "#shap.force_plot(explainer.expected_value[0], shap_values[0], X.iloc[299, :])\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[1], keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[2], shap_values[2], keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[3], shap_values[3], keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[4], shap_values[4], keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot(explainer.expected_value[5], shap_values[5], X.iloc[299, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values50 = explainer.shap_values(X.iloc[280:330, :], nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values50[0], X.iloc[280:330, :],feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[4], shap_values50[4], X.iloc[280:330, :],feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values50[0], X.iloc[280:330, :], plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values50[2], X, plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values50[4], X.iloc[280:330, :], plot_type=\"bar\", feature_names=keyword)\n",
    "print('shap_values50[4]: ', shap_values50[4].shape, shap_values50[4].sum(), shap_values50[4].min(), shap_values50[4].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap_values11 = explainer.shap_values(X.iloc[88888:88899, :], nsamples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values11[0], X.iloc[280:291, :], plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values, shap_values50, shap_values11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_abs = np.abs(shap_values)\n",
    "#shap_values_abs = np.abs(shap_values50)\n",
    "shap_values_abs_sum = np.sum(shap_values_abs, axis=0)\n",
    "shap_values_abs_sum_argsort = np.argsort(shap_values_abs_sum)\n",
    "print(shap_values_abs, shap_values_abs_sum, shap_values_abs_sum_argsort)\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
    "print('\\n', feature_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2, 3, 4, 5, 6, -7], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7]]\n",
    "nd_a = np.array(a)\n",
    "\n",
    "#shap_values = nd_a\n",
    "shap_values_abs = np.abs(shap_values)\n",
    "#shap_values_abs = np.abs(shap_values50)\n",
    "shap_values_abs_sum = np.sum(shap_values_abs, axis=0)\n",
    "shap_values_abs_sum_argsort = np.argsort(shap_values_abs_sum)\n",
    "print(\"\\nshap_values: (shap_values)\\n\", shap_values, shap_values.dtype, shap_values.shape, \"\\nshap_values_abs: np.abs(shap_values)\\n\", shap_values_abs, \n",
    "      \"\\nshap_values_abs_sum: np.sum(shap_values_abs, axis=0)\\n\", shap_values_abs_sum, \"\\nshap_values_abs_sum_argsort: np.argsort(shap_values_abs_sum)\\n\", shap_values_abs_sum_argsort)\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
    "print('\\nfeature_order:', feature_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.values[00,7)\n",
    "shap00_list = [-0.00537375,  0.01205069,  0.04364897,  0.00142398, -0.00139138, -0.08626259,  0.01085853]\n",
    "shap00_array = np.array(shap00_list)\n",
    "print(shap00_array.dtype, shap00_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[-9.,11.,-21.,63.,-252.],[3891.,506.,-1008.,3031.,-12117.],[3891.,576.,-1149.,3451.,-13801.],[3891.,-3891.,7782.,-23345.,93365.],[1024.,-1024.,2049.,-6144.,24572.]])\n",
    "x = np.abs(B).argmax(axis=0)[0]\n",
    "print(B, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_abs = np.abs(shap_values)\n",
    "#shap_values_abs = np.abs(shap_values50)\n",
    "shap_values_abs_sum = np.sum(shap_values_abs, axis=0)\n",
    "shap_values_abs_sum_argsort = np.argsort(shap_values_abs_sum)\n",
    "print(shap_values, shap_values_abs, shap_values_abs_sum, shap_values_abs_sum_argsort)\n",
    "feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
    "print('\\n', feature_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values50[3], X.iloc[280:330, :], plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values50, X, plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values[1].sum(), X.shape)\n",
    "#shap.summary_plot(shap_values, X, plot_type=\"bar\", feature_names=keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values50[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==== MAIN THREAD FROM 11/22: shap.KernelExplainer(model.predict, denseTest[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.file_x_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_generator.file_x_stack\n",
    "y_train = test_generator.file_x_stack[0]\n",
    "print(X_train)\n",
    "print(X_train[0])\n",
    "print(X_train[3].shape)\n",
    "print(X_train.reshape(1, -1))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_summary = shap.kmeans(train_generator.file_x_stack, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_summary.data.shape)\n",
    "print(X_train_summary.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_summary.data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_data(val, keep_index=False):\n",
    "#    if isinstance(val, shap.common.Data):\n",
    "#        return val\n",
    "    if type(val) == np.ndarray:\n",
    "        return DenseData(val, [str(i) for i in range(val.shape[1])])\n",
    "    elif str(type(val)).endswith(\"'pandas.core.series.Series'>\"):\n",
    "        return DenseData(val.values.reshape((1,len(val))), list(val.index))\n",
    "    elif str(type(val)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n",
    "        if keep_index:\n",
    "            return DenseDataWithIndex(val.values, list(val.columns), val.index.values, val.index.name)\n",
    "        else:\n",
    "            return DenseData(val.values, list(val.columns))\n",
    "    elif sp.sparse.issparse(val):\n",
    "        if not sp.sparse.isspmatrix_csr(val):\n",
    "            val = val.tocsr()\n",
    "        return SparseData(val)\n",
    "    else:\n",
    "        assert False, \"Unknown type passed as data object: \"+str(type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))\n",
    "print(model.name)\n",
    "print(model.input)\n",
    "print(model.output)\n",
    "print(model.input.shape)\n",
    "print(model.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = test_generator.file_x_stack\n",
    "print(val.shape[1], range(val.shape[1]))\n",
    "denseTest = shap.utils._legacy.DenseData(val, [str(i) for i in range(val.shape[1])])\n",
    "print(denseTest.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(X):\n",
    "    return model.predict([X[:, i] for i in range(X.shape[1])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(my_predict, model.layers[0].input)\n",
    "#explainer = shap.KernelExplainer(model.predict, model.layers[0].input)\n",
    "\n",
    "#explainer = shap.KernelExplainer(model.predict, denseTest[0][0])\n",
    "#explainer = shap.KernelExplainer(model.predict, test_generator.file_x_stack[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(predictions.shape)\n",
    "print(predictions.dtype)\n",
    "print(predictions.min())\n",
    "print(predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from multi_path_fusion.src.data_generators.mpf_h_data_generator import MPF_H_DataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#nn = MLPRegressor(solver=\"lbfgs\", alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=0)\n",
    "#nn.fit(X_train.reshape(1, -1), y_train)\n",
    "#print_accuracy(nn.predict)\n",
    "\n",
    "# explain all the predictions in the test set\n",
    "#explainer = shap.KernelExplainer(model.predict, test_generator)\n",
    "explainer = shap.KernelExplainer(model.predict, X_train_summary.data)\n",
    "#test_generator\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "#shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X_train, X_train_summary )\n",
    "\n",
    "print_accuracy(lin_regr.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "print(X)\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(y, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape = (2, 5) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x[0:])\n",
    "print(x[0:,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpf_bands = np.array([18, 14, 9, 3, 20, 8, 10])\n",
    "mpf_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.batch_size)\n",
    "print(train_generator.branch_inputs)\n",
    "print(len(train_generator.file_x_stack))\n",
    "print(train_generator.file_x_stack.shape)\n",
    "print(train_generator.file_x_stack[0].shape)\n",
    "print(type(train_generator.file_x_stack))\n",
    "print(train_generator.file_x_stack[0][799999])\n",
    "print(train_generator.file_x_stack[3,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_generator.file_x_stack\n",
    "df=pd.DataFrame(data=data[0:,0:], \n",
    "                index=[i for i in range(data.shape[0])],\n",
    "                columns=['f'+str(i) for i in range(data.shape[1])])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_generator.file_x_stack\n",
    "print(data.shape)\n",
    "print(type(data))\n",
    "df=pd.DataFrame(data=data[0:,0:], \n",
    "                index=[i for i in range(data.shape[0])],\n",
    "                columns=['f'+str(i) for i in range(data.shape[1])])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_generator.batch_size)\n",
    "print(test_generator.branch_inputs)\n",
    "print(len(test_generator.file_x_stack))\n",
    "print(test_generator.file_x_stack.shape)\n",
    "print(test_generator.file_x_stack[0].shape)\n",
    "print(type(test_generator.file_x_stack))\n",
    "print(test_generator.file_x_stack[0][99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_generator.file_x_stack\n",
    "df=pd.DataFrame(data=data[0:,0:], \n",
    "                index=[i for i in range(data.shape[0])],\n",
    "                columns=['f'+str(i) for i in range(data.shape[1])])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (train_generator.file_x_stack).astype(float)\n",
    "X\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.batch_size)\n",
    "print(train_generator.branch_inputs)\n",
    "print(len(train_generator.file_x_stack))\n",
    "print(train_generator.file_x_stack.shape)\n",
    "print(train_generator.file_x_stack[0].shape)\n",
    "print(type(train_generator.file_x_stack))\n",
    "print(train_generator.file_x_stack[0][799999])\n",
    "print(train_generator.file_x_stack[4,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn = MLPRegressor(solver=\"lbfgs\", alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=0)\n",
    "nn.fit(X_train, y_train)\n",
    "print_accuracy(nn.predict)\n",
    "\n",
    "# explain all the predictions in the test set\n",
    "explainer = shap.KernelExplainer(nn.predict, X_train_summary)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict, X_train_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test,nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return model.predict([X[:, i] for i in range(X.shape[1])]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(f, test_generator.file_x_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(f, X.iloc[:50, :])\n",
    "shap_values = explainer.shap_values(X.iloc[299, :], nsamples=500)\n",
    "shap.force_plot(explainer.expected_value, shap_values, X_display.iloc[299, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('mpf_sequential_hd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.models.load_model('mpf_sequential_hd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * SHAP * it performs a perturbation around the points of the training dataset and calculates the impact of this perturbation to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.yourdatateacher.com/2021/05/17/how-to-explain-neural-networks-using-shap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)\n",
    "features = load_diabetes()['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(5,),activation='logistic', max_iter=10000,learning_rate='invscaling',random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model.predict,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "print(len(X_test))\n",
    "print(X_test.mean())\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test,nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,X_test,feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:]  ,X_test[0,:],feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab-tensorflow]",
   "language": "python",
   "name": "conda-env-ilab-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
