{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "# Distributed - spread your data and computation across a cluster\n",
    "\n",
    "As we covered at the beginning Dask has the ability to run work on multiple machines using the distributed scheduler.\n",
    "\n",
    "Until now we have actually been using the distributed scheduler for our work, but just on a single machine.\n",
    "\n",
    "When we instantiate a `Client()` object with no arguments it will attempt to locate a Dask cluster. It will check your local Dask config and environment variables to see if connection information has been specified. If not it will create an instance of `LocalCluster` and use that.\n",
    "\n",
    "*Specifying connection information in config is useful for system administrators to provide access to their users. We do this in the [Dask Helm Chart for Kubernetes](https://github.com/dask/helm-chart/blob/master/dask/templates/dask-jupyter-deployment.yaml#L46-L48), the chart installs a multi-node Dask cluster and a Jupyter server on a Kubernetes cluster and Jupyter is preconfigured to discover the distributed cluster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38455 instead\n",
      "  warnings.warn(\n",
      "/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "clientDefault = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:38455/status'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientDefault.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = clientDefault\n",
    "workers = list(c.scheduler_info()['workers'])\n",
    "print(c.scheduler_info(), workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from dask.distributed import Client\n",
    "\n",
    "client = clientDefault\n",
    "#client = Client()  # normal blocking client\n",
    "\n",
    "async def f():\n",
    "    future = client.submit(lambda x: x + 1, 10)\n",
    "    result = await client.gather(future, asynchronous=True)\n",
    "    return result\n",
    "\n",
    "client.sync(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientDefault.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 13:05:24,308 - distributed.protocol.core - CRITICAL - Failed to deserialize\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py\", line 158, in loads\n",
      "    return msgpack.loads(\n",
      "  File \"msgpack/_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py\", line 138, in _decode_default\n",
      "    return merge_and_deserialize(\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 497, in merge_and_deserialize\n",
      "    return deserialize(header, merged_frames, deserializers=deserializers)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 426, in deserialize\n",
      "    return loads(header, frames)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 180, in serialization_error_loads\n",
      "    raise TypeError(msg)\n",
      "TypeError: Could not serialize object of type coroutine\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n",
      "    result = pickle.dumps(x, **dump_kwargs)\n",
      "TypeError: cannot pickle 'coroutine' object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n",
      "    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n",
      "    frames[0] = pickle.dumps(\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n",
      "    result = cloudpickle.dumps(x, **dump_kwargs)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
      "    return super().dump(obj)\n",
      "TypeError: cannot pickle 'coroutine' object\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not serialize object of type coroutine\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nTypeError: cannot pickle 'coroutine' object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n    cp.dump(obj)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\nTypeError: cannot pickle 'coroutine' object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdask_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtcp://127.0.0.1:46143\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretire_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:2788\u001b[0m, in \u001b[0;36mClient.run_on_scheduler\u001b[0;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_on_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a function on the scheduler process\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m \n\u001b[1;32m   2754\u001b[0m \u001b[38;5;124;03m    This is typically used for live debugging.  The function should take a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03m    Client.run : Run a function on all workers\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_on_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:338\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:405\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    404\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:378\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    377\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:2739\u001b[0m, in \u001b[0;36mClient._run_on_scheduler\u001b[0;34m(self, function, wait, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_on_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, function, \u001b[38;5;241m*\u001b[39margs, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2739\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mrun_function(\n\u001b[1;32m   2740\u001b[0m         function\u001b[38;5;241m=\u001b[39mdumps(function),\n\u001b[1;32m   2741\u001b[0m         args\u001b[38;5;241m=\u001b[39mdumps(args),\n\u001b[1;32m   2742\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mdumps(kwargs),\n\u001b[1;32m   2743\u001b[0m         wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m   2744\u001b[0m     )\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2746\u001b[0m         typ, exc, tb \u001b[38;5;241m=\u001b[39m clean_exception(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/core.py:1221\u001b[0m, in \u001b[0;36mPooledRPCCall.__getattr__.<locals>.send_recv_from_rpc\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m prev_name, comm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnectionPool.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m key\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m send_recv(comm\u001b[38;5;241m=\u001b[39mcomm, op\u001b[38;5;241m=\u001b[39mkey, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mreuse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddr, comm)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/core.py:986\u001b[0m, in \u001b[0;36msend_recv\u001b[0;34m(comm, reply, serializers, deserializers, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mwrite(msg, serializers\u001b[38;5;241m=\u001b[39mserializers, on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply:\n\u001b[0;32m--> 986\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mread(deserializers\u001b[38;5;241m=\u001b[39mdeserializers)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py:254\u001b[0m, in \u001b[0;36mTCP.read\u001b[0;34m(self, deserializers)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     frames \u001b[38;5;241m=\u001b[39m unpack_frames(frames)\n\u001b[0;32m--> 254\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m from_frames(\n\u001b[1;32m    255\u001b[0m         frames,\n\u001b[1;32m    256\u001b[0m         deserialize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeserialize,\n\u001b[1;32m    257\u001b[0m         deserializers\u001b[38;5;241m=\u001b[39mdeserializers,\n\u001b[1;32m    258\u001b[0m         allow_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_offload,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# Frames possibly garbled or truncated by communication error\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabort()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/utils.py:100\u001b[0m, in \u001b[0;36mfrom_frames\u001b[0;34m(frames, deserialize, deserializers, allow_offload)\u001b[0m\n\u001b[1;32m     98\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m offload(_from_frames)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_from_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/utils.py:83\u001b[0m, in \u001b[0;36mfrom_frames.<locals>._from_frames\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_frames\u001b[39m():\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserialize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py:158\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(frames, deserialize, deserializers)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickle on the Scheduler isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt allowed, set `distributed.scheduler.pickle=true`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m msgpack_decode_default(obj)\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmsgpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_decode_default\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsgpack_opts\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/msgpack/_unpacker.pyx:194\u001b[0m, in \u001b[0;36mmsgpack._cmsgpack.unpackb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py:138\u001b[0m, in \u001b[0;36mloads.<locals>._decode_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sub_header:\n\u001b[1;32m    137\u001b[0m         sub_frames \u001b[38;5;241m=\u001b[39m decompress(sub_header, sub_frames)\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_and_deserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Serialized(sub_header, sub_frames)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:497\u001b[0m, in \u001b[0;36mmerge_and_deserialize\u001b[0;34m(header, frames, deserializers)\u001b[0m\n\u001b[1;32m    493\u001b[0m             merged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\u001b[38;5;241m.\u001b[39mjoin(subframes)\n\u001b[1;32m    495\u001b[0m         merged_frames\u001b[38;5;241m.\u001b[39mappend(merged)\n\u001b[0;32m--> 497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:426\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(header, frames, deserializers)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData serialized with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but only able to deserialize \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlist\u001b[39m(deserializers)))\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m dumps, loads, wants_context \u001b[38;5;241m=\u001b[39m families[name]\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:180\u001b[0m, in \u001b[0;36mserialization_error_loads\u001b[0;34m(header, frames)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialization_error_loads\u001b[39m(header, frames):\n\u001b[1;32m    179\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([codecs\u001b[38;5;241m.\u001b[39mdecode(frame, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames])\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not serialize object of type coroutine\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nTypeError: cannot pickle 'coroutine' object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n    cp.dump(obj)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\nTypeError: cannot pickle 'coroutine' object\n"
     ]
    }
   ],
   "source": [
    "c.run_on_scheduler(lambda dask_scheduler=None: \n",
    "    dask_scheduler.retire_workers(workers, close_workers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tcp://127.0.0.1:37495'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " workers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_current\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "print(c.as_current().get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GFX1-]: More than 2 GPUs detected via PCI, secondary GPU is arbitrary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DISPLAY\"] = \"localhost:10.0\"\n",
    "\n",
    "url = 'http://docs.python.org/'\n",
    "webbrowser.get(\"/explore/nobackup/people/gtamkin/dev/browsers/firefox/firefox -url %s\").open(\"http://google.com\")\n",
    "#firefox_path = '/explore/nobackup/people/gtamkin/dev/browsers/firefox/firefox'\n",
    "#webbrowser.get(firefox_path).open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:39141/status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "print(clientDefault.dashboard_link)\n",
    "webbrowser.open(clientDefault.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientDefault.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientDefault.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 11:37:13,459 - distributed.protocol.core - CRITICAL - Failed to deserialize\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py\", line 158, in loads\n",
      "    return msgpack.loads(\n",
      "  File \"msgpack/_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py\", line 138, in _decode_default\n",
      "    return merge_and_deserialize(\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 497, in merge_and_deserialize\n",
      "    return deserialize(header, merged_frames, deserializers=deserializers)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 426, in deserialize\n",
      "    return loads(header, frames)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 180, in serialization_error_loads\n",
      "    raise TypeError(msg)\n",
      "TypeError: Could not serialize object of type coroutine\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n",
      "    result = pickle.dumps(x, **dump_kwargs)\n",
      "TypeError: cannot pickle 'coroutine' object\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n",
      "    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n",
      "    frames[0] = pickle.dumps(\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n",
      "    result = cloudpickle.dumps(x, **dump_kwargs)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n",
      "    return super().dump(obj)\n",
      "TypeError: cannot pickle 'coroutine' object\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not serialize object of type coroutine\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nTypeError: cannot pickle 'coroutine' object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n    cp.dump(obj)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\nTypeError: cannot pickle 'coroutine' object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m clientDefault\n\u001b[1;32m      2\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(c\u001b[38;5;241m.\u001b[39mscheduler_info()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkers\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdask_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretire_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:2788\u001b[0m, in \u001b[0;36mClient.run_on_scheduler\u001b[0;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_on_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a function on the scheduler process\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m \n\u001b[1;32m   2754\u001b[0m \u001b[38;5;124;03m    This is typically used for live debugging.  The function should take a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03m    Client.run : Run a function on all workers\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_on_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:338\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:405\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    404\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:378\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    377\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:2739\u001b[0m, in \u001b[0;36mClient._run_on_scheduler\u001b[0;34m(self, function, wait, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_on_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, function, \u001b[38;5;241m*\u001b[39margs, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2739\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mrun_function(\n\u001b[1;32m   2740\u001b[0m         function\u001b[38;5;241m=\u001b[39mdumps(function),\n\u001b[1;32m   2741\u001b[0m         args\u001b[38;5;241m=\u001b[39mdumps(args),\n\u001b[1;32m   2742\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mdumps(kwargs),\n\u001b[1;32m   2743\u001b[0m         wait\u001b[38;5;241m=\u001b[39mwait,\n\u001b[1;32m   2744\u001b[0m     )\n\u001b[1;32m   2745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2746\u001b[0m         typ, exc, tb \u001b[38;5;241m=\u001b[39m clean_exception(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/core.py:1221\u001b[0m, in \u001b[0;36mPooledRPCCall.__getattr__.<locals>.send_recv_from_rpc\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m prev_name, comm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnectionPool.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m key\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m send_recv(comm\u001b[38;5;241m=\u001b[39mcomm, op\u001b[38;5;241m=\u001b[39mkey, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mreuse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddr, comm)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/core.py:986\u001b[0m, in \u001b[0;36msend_recv\u001b[0;34m(comm, reply, serializers, deserializers, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mwrite(msg, serializers\u001b[38;5;241m=\u001b[39mserializers, on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply:\n\u001b[0;32m--> 986\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mread(deserializers\u001b[38;5;241m=\u001b[39mdeserializers)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py:254\u001b[0m, in \u001b[0;36mTCP.read\u001b[0;34m(self, deserializers)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     frames \u001b[38;5;241m=\u001b[39m unpack_frames(frames)\n\u001b[0;32m--> 254\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m from_frames(\n\u001b[1;32m    255\u001b[0m         frames,\n\u001b[1;32m    256\u001b[0m         deserialize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeserialize,\n\u001b[1;32m    257\u001b[0m         deserializers\u001b[38;5;241m=\u001b[39mdeserializers,\n\u001b[1;32m    258\u001b[0m         allow_offload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_offload,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# Frames possibly garbled or truncated by communication error\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabort()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/utils.py:100\u001b[0m, in \u001b[0;36mfrom_frames\u001b[0;34m(frames, deserialize, deserializers, allow_offload)\u001b[0m\n\u001b[1;32m     98\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m offload(_from_frames)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_from_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/utils.py:83\u001b[0m, in \u001b[0;36mfrom_frames.<locals>._from_frames\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_frames\u001b[39m():\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserialize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py:158\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(frames, deserialize, deserializers)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickle on the Scheduler isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt allowed, set `distributed.scheduler.pickle=true`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m msgpack_decode_default(obj)\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmsgpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_decode_default\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsgpack_opts\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/msgpack/_unpacker.pyx:194\u001b[0m, in \u001b[0;36mmsgpack._cmsgpack.unpackb\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/core.py:138\u001b[0m, in \u001b[0;36mloads.<locals>._decode_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sub_header:\n\u001b[1;32m    137\u001b[0m         sub_frames \u001b[38;5;241m=\u001b[39m decompress(sub_header, sub_frames)\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_and_deserialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43msub_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Serialized(sub_header, sub_frames)\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:497\u001b[0m, in \u001b[0;36mmerge_and_deserialize\u001b[0;34m(header, frames, deserializers)\u001b[0m\n\u001b[1;32m    493\u001b[0m             merged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\u001b[38;5;241m.\u001b[39mjoin(subframes)\n\u001b[1;32m    495\u001b[0m         merged_frames\u001b[38;5;241m.\u001b[39mappend(merged)\n\u001b[0;32m--> 497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserializers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:426\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m(header, frames, deserializers)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData serialized with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but only able to deserialize \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (name, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlist\u001b[39m(deserializers)))\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m dumps, loads, wants_context \u001b[38;5;241m=\u001b[39m families[name]\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py:180\u001b[0m, in \u001b[0;36mserialization_error_loads\u001b[0;34m(header, frames)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserialization_error_loads\u001b[39m(header, frames):\n\u001b[1;32m    179\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([codecs\u001b[38;5;241m.\u001b[39mdecode(frame, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames])\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not serialize object of type coroutine\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 46, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\nTypeError: cannot pickle 'coroutine' object\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 347, in serialize\n    header, frames = dumps(x, context=context) if wants_context else dumps(x)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/serialize.py\", line 71, in pickle_dumps\n    frames[0] = pickle.dumps(\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/protocol/pickle.py\", line 58, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\n    cp.dump(obj)\n  File \"/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\nTypeError: cannot pickle 'coroutine' object\n"
     ]
    }
   ],
   "source": [
    "c = clientDefault\n",
    "workers = list(c.scheduler_info()['workers'])\n",
    "c.run_on_scheduler(lambda dask_scheduler=None: \n",
    "    dask_scheduler.retire_workers(workers, close_workers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp://127.0.0.1:34505',\n",
       " 'tcp://127.0.0.1:34991',\n",
       " 'tcp://127.0.0.1:35537',\n",
       " 'tcp://127.0.0.1:42635',\n",
       " 'tcp://127.0.0.1:42733']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientDefault.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clientDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Timed out during handshake while connecting to tcp://127.0.0.1:8797 after 30 s",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/tcp.py:225\u001b[0m, in \u001b[0;36mTCP.read\u001b[0;34m(self, deserializers)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     frames_nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mread_bytes(fmt_size)\n\u001b[1;32m    226\u001b[0m     (frames_nbytes,) \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(fmt, frames_nbytes)\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/asyncio/tasks.py:456\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/core.py:328\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# This would be better, but connections leak if worker is closed quickly\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# write, handshake = await asyncio.gather(comm.write(local_info), comm.read())\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     handshake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(comm\u001b[38;5;241m.\u001b[39mread(), time_left())\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(comm\u001b[38;5;241m.\u001b[39mwrite(local_info), time_left())\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/asyncio/tasks.py:458\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 458\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, LocalCluster\n\u001b[0;32m----> 2\u001b[0m clientL \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m127.0.0.1:8797\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:991\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, address, loop, timeout, set_as_default, scheduler_file, security, asynchronous, name, heartbeat_interval, serializers, deserializers, extensions, direct_to_workers, connection_limit, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m preload_argv \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributed.client.preload-argv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreloads \u001b[38;5;241m=\u001b[39m preloading\u001b[38;5;241m.\u001b[39mprocess_preloads(\u001b[38;5;28mself\u001b[39m, preload, preload_argv)\n\u001b[0;32m--> 991\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m Client\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecreate_tasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReplayTaskClient\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:1188\u001b[0m, in \u001b[0;36mClient.start\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_started \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:405\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    404\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/utils.py:378\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[1;32m    377\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/tornado/gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:1268\u001b[0m, in \u001b[0;36mClient._start\u001b[0;34m(self, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_connected(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close()\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:1331\u001b[0m, in \u001b[0;36mClient._ensure_connected\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connecting_to_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1331\u001b[0m     comm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connect(\n\u001b[1;32m   1332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39maddress, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_args\n\u001b[1;32m   1333\u001b[0m     )\n\u001b[1;32m   1334\u001b[0m     comm\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient->Scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/comm/core.py:333\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(addr, timeout, deserialize, handshake_overrides, **connection_args)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out during handshake while connecting to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    337\u001b[0m comm\u001b[38;5;241m.\u001b[39mremote_info \u001b[38;5;241m=\u001b[39m handshake\n\u001b[1;32m    338\u001b[0m comm\u001b[38;5;241m.\u001b[39mremote_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39m_peer_addr\n",
      "\u001b[0;31mOSError\u001b[0m: Timed out during handshake while connecting to tcp://127.0.0.1:8797 after 30 s"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "clientL = Client('127.0.0.1:8797')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want to use an asynchronous function with a synchronous Client (one made without the asynchronous=True keyword) then you can apply the asynchronous=True keyword at each method call and use the Client.sync function to run the asynchronous function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLocalCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthreads_per_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msilence_logs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdashboard_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m':8787'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworker_dashboard_address\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdiagnostics_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mservices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworker_services\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mservice_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msecurity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblocked_handlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minterface\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworker_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_sync_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mworker_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create local Scheduler and Workers\n",
       "\n",
       "This creates a \"cluster\" of a scheduler and workers running on the local\n",
       "machine.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_workers: int\n",
       "    Number of workers to start\n",
       "memory_limit: str, float, int, or None, default \"auto\"\n",
       "    Sets the memory limit *per worker*.\n",
       "\n",
       "    Notes regarding argument data type:\n",
       "\n",
       "    * If None or 0, no limit is applied.\n",
       "    * If \"auto\", the total system memory is split evenly between the workers.\n",
       "    * If a float, that fraction of the system memory is used *per worker*.\n",
       "    * If a string giving a number of bytes (like ``\"1GiB\"``), that amount is used *per worker*.\n",
       "    * If an int, that number of bytes is used *per worker*.\n",
       "\n",
       "    Note that the limit will only be enforced when ``processes=True``, and the limit is only\n",
       "    enforced on a best-effort basis  it's still possible for workers to exceed this limit.\n",
       "processes: bool\n",
       "    Whether to use processes (True) or threads (False).  Defaults to True, unless\n",
       "    worker_class=Worker, in which case it defaults to False.\n",
       "threads_per_worker: int\n",
       "    Number of threads per each worker\n",
       "scheduler_port: int\n",
       "    Port of the scheduler.  8786 by default, use 0 to choose a random port\n",
       "silence_logs: logging level\n",
       "    Level of logs to print out to stdout.  ``logging.WARN`` by default.\n",
       "    Use a falsey value like False or None for no change.\n",
       "host: string\n",
       "    Host address on which the scheduler will listen, defaults to only localhost\n",
       "ip: string\n",
       "    Deprecated.  See ``host`` above.\n",
       "dashboard_address: str\n",
       "    Address on which to listen for the Bokeh diagnostics server like\n",
       "    'localhost:8787' or '0.0.0.0:8787'.  Defaults to ':8787'.\n",
       "    Set to ``None`` to disable the dashboard.\n",
       "    Use ':0' for a random port.\n",
       "worker_dashboard_address: str\n",
       "    Address on which to listen for the Bokeh worker diagnostics server like\n",
       "    'localhost:8787' or '0.0.0.0:8787'.  Defaults to None which disables the dashboard.\n",
       "    Use ':0' for a random port.\n",
       "diagnostics_port: int\n",
       "    Deprecated.  See dashboard_address.\n",
       "asynchronous: bool (False by default)\n",
       "    Set to True if using this cluster within async/await functions or within\n",
       "    Tornado gen.coroutines.  This should remain False for normal use.\n",
       "blocked_handlers: List[str]\n",
       "    A list of strings specifying a blocklist of handlers to disallow on the\n",
       "    Scheduler, like ``['feed', 'run_function']``\n",
       "service_kwargs: Dict[str, Dict]\n",
       "    Extra keywords to hand to the running services\n",
       "security : Security or bool, optional\n",
       "    Configures communication security in this cluster. Can be a security\n",
       "    object, or True. If True, temporary self-signed credentials will\n",
       "    be created automatically.\n",
       "protocol: str (optional)\n",
       "    Protocol to use like ``tcp://``, ``tls://``, ``inproc://``\n",
       "    This defaults to sensible choice given other keyword arguments like\n",
       "    ``processes`` and ``security``\n",
       "interface: str (optional)\n",
       "    Network interface to use.  Defaults to lo/localhost\n",
       "worker_class: Worker\n",
       "    Worker class used to instantiate workers from. Defaults to Worker if\n",
       "    processes=False and Nanny if processes=True or omitted.\n",
       "**worker_kwargs:\n",
       "    Extra worker arguments. Any additional keyword arguments will be passed\n",
       "    to the ``Worker`` class constructor.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> cluster = LocalCluster()  # Create a local cluster  # doctest: +SKIP\n",
       ">>> cluster  # doctest: +SKIP\n",
       "LocalCluster(\"127.0.0.1:8786\", workers=8, threads=8)\n",
       "\n",
       ">>> c = Client(cluster)  # connect to local cluster  # doctest: +SKIP\n",
       "\n",
       "Scale the cluster to three workers\n",
       "\n",
       ">>> cluster.scale(3)  # doctest: +SKIP\n",
       "\n",
       "Pass extra keyword arguments to Bokeh\n",
       "\n",
       ">>> LocalCluster(service_kwargs={'dashboard': {'prefix': '/foo'}})  # doctest: +SKIP\n",
       "\u001b[0;31mFile:\u001b[0m           /explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/deploy/local.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "?LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mgather(future, asynchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 10\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39msync(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "#from dask.distributed import Client\n",
    "\n",
    "#client = Client()  # normal blocking client\n",
    "\n",
    "async def f():\n",
    "    future = client.submit(lambda x: x + 1, 10)\n",
    "    result = await client.gather(future, asynchronous=True)\n",
    "    return result\n",
    "\n",
    "client.sync(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSH Cluster\n",
    "\n",
    "Let's explore the `SSHCluster` object ourselves and see what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 08:51:05,257 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:05,256 - distributed.scheduler - INFO - State start\n",
      "2024-01-28 08:51:05,264 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:05,263 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.100.38.1:46601\n",
      "2024-01-28 08:51:06,946 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:06,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.100.38.1:44037'\n",
      "2024-01-28 08:51:06,959 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:06,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.100.38.1:36409'\n",
      "2024-01-28 08:51:06,960 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:06,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.100.38.1:40295'\n",
      "2024-01-28 08:51:06,962 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:06,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.100.38.1:40623'\n",
      "2024-01-28 08:51:07,552 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,550 - distributed.worker - INFO -       Start worker at:    tcp://10.100.38.1:36677\n",
      "2024-01-28 08:51:07,553 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,550 - distributed.worker - INFO -       Start worker at:    tcp://10.100.38.1:41185\n",
      "2024-01-28 08:51:07,553 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,550 - distributed.worker - INFO -          Listening to:    tcp://10.100.38.1:41185\n",
      "2024-01-28 08:51:07,553 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,550 - distributed.worker - INFO -          Listening to:    tcp://10.100.38.1:36677\n",
      "2024-01-28 08:51:07,554 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -          dashboard at:          10.100.38.1:39375\n",
      "2024-01-28 08:51:07,554 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -          dashboard at:          10.100.38.1:43661\n",
      "2024-01-28 08:51:07,554 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - Waiting to connect to:    tcp://10.100.38.1:46601\n",
      "2024-01-28 08:51:07,555 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - Waiting to connect to:    tcp://10.100.38.1:46601\n",
      "2024-01-28 08:51:07,555 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,556 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,556 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-01-28 08:51:07,556 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-01-28 08:51:07,557 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -                Memory:                  37.72 GiB\n",
      "2024-01-28 08:51:07,557 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -                Memory:                  37.72 GiB\n",
      "2024-01-28 08:51:07,558 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -       Local Directory: /explore/nobackup/people/gtamkin/.nccstmp/dask-worker-space/worker-ucimwezf\n",
      "2024-01-28 08:51:07,558 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO -       Local Directory: /explore/nobackup/people/gtamkin/.nccstmp/dask-worker-space/worker-x9j061iy\n",
      "2024-01-28 08:51:07,560 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,560 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,551 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,561 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -       Start worker at:    tcp://10.100.38.1:44435\n",
      "2024-01-28 08:51:07,561 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -          Listening to:    tcp://10.100.38.1:44435\n",
      "2024-01-28 08:51:07,561 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -          dashboard at:          10.100.38.1:37995\n",
      "2024-01-28 08:51:07,561 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO - Waiting to connect to:    tcp://10.100.38.1:46601\n",
      "2024-01-28 08:51:07,562 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,562 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -               Threads:                          2\n",
      "2024-01-28 08:51:07,562 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -                Memory:                  37.72 GiB\n",
      "2024-01-28 08:51:07,563 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO -       Local Directory: /explore/nobackup/people/gtamkin/.nccstmp/dask-worker-space/worker-4ydbnrtr\n",
      "2024-01-28 08:51:07,563 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,552 - distributed.worker - INFO - -------------------------------------------------\n",
      "2024-01-28 08:51:07,577 - distributed.deploy.ssh - INFO - 2024-01-28 08:51:07,576 - distributed.worker - INFO -       Start worker at:    tcp://10.100.38.1:39153\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, SSHCluster\n",
    "cluster = SSHCluster(\n",
    "#    [\"localhost(adaptjh3)\",\"ilab210\",\"ilab211\",\"ilab212\"],\n",
    "#    [\"10.100.128.32\",\"10.100.40.70\",\"10.100.140.71\",\"10.100.140.72\"],\n",
    "    [\"localhost\",\"localhost\"],\n",
    "    connect_options={\"known_hosts\": None},\n",
    "    worker_options={\"nthreads\": 2, \"n_workers\": 4},\n",
    "    scheduler_options={\"port\": 0, \"dashboard_address\": \":8797\"}\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>Future: lambda</strong>\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> status: </span>\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-error-color0, black)\">finished</span>,\n",
       "\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> type:</span> int,\n",
       "\n",
       "\n",
       "<span style=\"color: var(--jp-ui-font-color2, gray)\"> key:</span> lambda-2c3e9e56c8eec332f8d97eed85626095"
      ],
      "text/plain": [
       "<Future: pending, key: lambda-2c3e9e56c8eec332f8d97eed85626095>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = client.submit(lambda x: x + 1, 10)\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute '__await__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "File \u001b[0;32m/explore/nobackup/people/gtamkin/.conda/envs/dask-tutorial/lib/python3.10/site-packages/distributed/client.py:507\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__await__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__await__\u001b[39;49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute '__await__'"
     ]
    }
   ],
   "source": [
    "result = await future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = await Client(asynchronous=True)\n",
    "future = client.submit(lambda x: x + 1, 10)\n",
    "result = await future\n",
    "await client.close()\n",
    "print(result)\n",
    "\n",
    "# async def f():\n",
    "#     client = await Client(asynchronous=True)\n",
    "#     future = client.submit(lambda x: x + 1, 10)\n",
    "#     result = await future\n",
    "#     await client.close()\n",
    "#     return result\n",
    "\n",
    "# # Either use Tornado\n",
    "# from tornado.ioloop import IOLoop\n",
    "# IOLoop().run_sync(f)\n",
    "\n",
    "# # Or use asyncio\n",
    "# import asyncio\n",
    "# asyncio.get_event_loop().run_until_complete(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?SSHCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cluster object has attributes and methods which we can use to access information about our cluster. For instance we can get the log output from the scheduler and all the workers with the `get_logs()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the url that the Dask dashboard is being hosted at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for Dask to use our cluster we still need to create a `Client` object, but as we have already created a cluster we can pass that directly to our client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stuff, but don't forget to cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "        return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39msubmit(inc, \u001b[38;5;241m10\u001b[39m, pure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "x = client.submit(inc, 10, pure=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.result()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = client.map(inc, range(1000))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Cluster\n",
    "\n",
    "Let's explore the `LocalCluster` object ourselves and see what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a cluster object will create a Dask scheduler and a number of Dask workers. If no arguments are specified then it will autodetect the number of CPU cores your system has and the amount of memory and create workers to appropriately fill that.\n",
    "\n",
    "You can also specify these arguments yourself. Let's have a look at the docstring to see the options we have available.\n",
    "\n",
    "*These arguments can also be passed to `Client` and in the case where it creates a `LocalCluster` they will just be passed on down the line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cluster object has attributes and methods which we can use to access information about our cluster. For instance we can get the log output from the scheduler and all the workers with the `get_logs()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the url that the Dask dashboard is being hosted at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for Dask to use our cluster we still need to create a `Client` object, but as we have already created a cluster we can pass that directly to our client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote clusters via SSH\n",
    "\n",
    "A common way to distribute your work onto multiple machines is via SSH. Dask has a cluster manager which will handle creating SSH connections for you called `SSHCluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dask.distributed import SSHCluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing this cluster manager we need to pass a list of addresses, either hostnames or IP addresses, which we will SSH into and attempt to start a Dask scheduler or worker on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cluster = SSHCluster([\"localhost\", \"hostA\", \"hostB\"])\n",
    "cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create our `SSHCluster` object we have given a list of three hostnames.\n",
    "\n",
    "The first host in the list will be used as the scheduler, all other hosts will be used as workers. If you're on the same network it wouldn't be unreasonable to set your local machine as the scheduler and then use other machines as workers.\n",
    "\n",
    "If your servers are remote to you, in the cloud for instance, you may want the scheduler to be a remote machine too to avoid network bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable clusters\n",
    "\n",
    "Both of the clusters we have seen so far are fixed size clusters. We are either running locally and using all the resources in our machine, or we are using an explicit number of other machines via SSH.\n",
    "\n",
    "With some cluster managers it is possible to increase and descrease the number of workers either by calling `cluster.scale(n)` in your code where `n` is the desired number of workers. Or you can let Dask do this dynamically by calling `cluster.adapt(minimum=1, maximum=100)` where minimum and maximum are your preferred limits for Dask to abide to.\n",
    "\n",
    "It is always good to keep your minimum to at least 1 as Dask will start running work on a single worker in order to profile how long things take and extrapolate how many additional workers it thinks it needs. Getting new workers may take time depending on your setup so keeping this at 1 or above means this profilling will start immediately.\n",
    "\n",
    "We currently have cluster managers for [Kubernetes](https://kubernetes.dask.org/en/latest/), [Hadoop/Yarn](https://yarn.dask.org/en/latest/), [cloud platforms](https://cloudprovider.dask.org/en/latest/) and [batch systems including PBS, SLURM and SGE](http://jobqueue.dask.org/en/latest/).\n",
    "\n",
    "These cluster managers allow users who have access to resources such as these to bootstrap Dask clusters on to them. If an institution wishes to provide a central service that users can request Dask clusters from there is also [Dask Gateway](https://gateway.dask.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster components\n",
    "\n",
    "The minimum requirements for a functioning Dask cluster is a scheduler process and one worker process.\n",
    "\n",
    "We can start these processes manually via the CLI. Let's start with the scheduler.\n",
    "\n",
    "```console\n",
    "$ dask-scheduler                                                               \n",
    "2022-07-07 14:11:35,661 - distributed.scheduler - INFO - -----------------------------------------------\n",
    "2022-07-07 14:11:37,405 - distributed.scheduler - INFO - State start\n",
    "2022-07-07 14:11:37,408 - distributed.scheduler - INFO - -----------------------------------------------\n",
    "2022-07-07 14:11:37,409 - distributed.scheduler - INFO - Clear task state\n",
    "2022-07-07 14:11:37,409 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.51.100.80:8786\n",
    "2022-07-07 14:11:37,409 - distributed.scheduler - INFO -   dashboard at:                     :8787\n",
    "```\n",
    "\n",
    "Then we can connect a worker on the address that the scheduler is listening on.\n",
    "\n",
    "```console\n",
    "$ dask-worker tcp://10.51.100.80:8786 --nworkers=auto\n",
    "2022-07-07 14:12:53,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.51.100.80:58051'\n",
    "2022-07-07 14:12:53,922 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.51.100.80:58052'\n",
    "2022-07-07 14:12:53,924 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.51.100.80:58053'\n",
    "2022-07-07 14:12:53,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.51.100.80:58054'\n",
    "2022-07-07 14:12:55,222 - distributed.worker - INFO -       Start worker at:   tcp://10.51.100.80:58065\n",
    "2022-07-07 14:12:55,222 - distributed.worker - INFO -          Listening to:   tcp://10.51.100.80:58065\n",
    "2022-07-07 14:12:55,223 - distributed.worker - INFO -          dashboard at:         10.51.100.80:58068\n",
    "2022-07-07 14:12:55,223 - distributed.worker - INFO - Waiting to connect to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,223 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,223 - distributed.worker - INFO -               Threads:                          3\n",
    "2022-07-07 14:12:55,223 - distributed.worker - INFO -                Memory:                   4.00 GiB\n",
    "2022-07-07 14:12:55,224 - distributed.worker - INFO -       Local Directory: /Users/jtomlinson/Projects/dask/dask-tutorial/dask-worker-space/worker-hlvac6m5\n",
    "2022-07-07 14:12:55,225 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO -       Start worker at:   tcp://10.51.100.80:58066\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO -          Listening to:   tcp://10.51.100.80:58066\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO -          dashboard at:         10.51.100.80:58070\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO - Waiting to connect to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,227 - distributed.worker - INFO -               Threads:                          3\n",
    "2022-07-07 14:12:55,228 - distributed.worker - INFO -                Memory:                   4.00 GiB\n",
    "2022-07-07 14:12:55,228 - distributed.worker - INFO -       Local Directory: /Users/jtomlinson/Projects/dask/dask-tutorial/dask-worker-space/worker-e1suf_7o\n",
    "2022-07-07 14:12:55,229 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,231 - distributed.worker - INFO -       Start worker at:   tcp://10.51.100.80:58063\n",
    "2022-07-07 14:12:55,233 - distributed.worker - INFO -          Listening to:   tcp://10.51.100.80:58063\n",
    "2022-07-07 14:12:55,233 - distributed.worker - INFO -          dashboard at:         10.51.100.80:58067\n",
    "2022-07-07 14:12:55,233 - distributed.worker - INFO - Waiting to connect to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,233 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,234 - distributed.worker - INFO -               Threads:                          3\n",
    "2022-07-07 14:12:55,234 - distributed.worker - INFO -                Memory:                   4.00 GiB\n",
    "2022-07-07 14:12:55,235 - distributed.worker - INFO -       Local Directory: /Users/jtomlinson/Projects/dask/dask-tutorial/dask-worker-space/worker-oq39ihb4\n",
    "2022-07-07 14:12:55,236 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,246 - distributed.worker - INFO -         Registered to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,246 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,249 - distributed.core - INFO - Starting established connection\n",
    "2022-07-07 14:12:55,264 - distributed.worker - INFO -         Registered to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,264 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,267 - distributed.worker - INFO -         Registered to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,267 - distributed.core - INFO - Starting established connection\n",
    "2022-07-07 14:12:55,267 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,269 - distributed.core - INFO - Starting established connection\n",
    "2022-07-07 14:12:55,273 - distributed.worker - INFO -       Start worker at:   tcp://10.51.100.80:58064\n",
    "2022-07-07 14:12:55,273 - distributed.worker - INFO -          Listening to:   tcp://10.51.100.80:58064\n",
    "2022-07-07 14:12:55,273 - distributed.worker - INFO -          dashboard at:         10.51.100.80:58069\n",
    "2022-07-07 14:12:55,273 - distributed.worker - INFO - Waiting to connect to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,274 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,274 - distributed.worker - INFO -               Threads:                          3\n",
    "2022-07-07 14:12:55,275 - distributed.worker - INFO -                Memory:                   4.00 GiB\n",
    "2022-07-07 14:12:55,275 - distributed.worker - INFO -       Local Directory: /Users/jtomlinson/Projects/dask/dask-tutorial/dask-worker-space/worker-zfie55ku\n",
    "2022-07-07 14:12:55,276 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,299 - distributed.worker - INFO -         Registered to:    tcp://10.51.100.80:8786\n",
    "2022-07-07 14:12:55,300 - distributed.worker - INFO - -------------------------------------------------\n",
    "2022-07-07 14:12:55,302 - distributed.core - INFO - Starting established connection\n",
    "```\n",
    "\n",
    "Then in Python we can connect a client to this cluster and submit some work.\n",
    "\n",
    "```python\n",
    ">>> from dask.distributed import Client\n",
    ">>> client = Client(\"tcp://10.51.100.80:8786\")\n",
    ">>> client.submit(lambda: 1+1)\n",
    "```\n",
    "\n",
    "We can also do this in Python by importing cluster components and creating them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Scheduler, Worker, Client\n",
    "\n",
    "async with Scheduler() as scheduler:\n",
    "    async with Worker(scheduler.address) as worker:\n",
    "        async with Client(scheduler.address, asynchronous=True) as client:\n",
    "            print(await client.submit(lambda: 1 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time we never have to create these components ourselves and instead can rely on cluster manager objects to do this for us. But in some situations it can be useful to be able to contruct a cluster yourself manually.\n",
    "\n",
    "You may also see a `Nanny` process being referenced from time to time. This is a wrapper for the worker that handles restarting the process if it is killed. When we run `dask-worker` via the CLI a nanny is automatically created for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster networking\n",
    "\n",
    "By default Dask uses a custom TCP based remote procedure call protocol to communicate between processes. The scheduler and workers all listen on TCP ports for communication.\n",
    "\n",
    "When you start a scheduler it typically listens on port `8786`. When a worker is created it listens on a random high port and communicates that port to the scheduler when it first connects.\n",
    "\n",
    "The scheduler maintains a list of all workers and their address which can be accessed via the workers, therefore both the scheduler and any of the workers can open connections to any other worker at any time. Connections are closed automatically when not in use.\n",
    "\n",
    "The `Client` will only ever connect to the scheduler and all communication to the workers will pass through it. This means that when deploying Dask clusters the scheduler and workers must typically be on the same network and able to access each other via IP and port directly. But the client can run wherever as long as it can access the scheduler communication port. It is common to configure firewall rules or load balancers to provide access to just the scheduler port.\n",
    "\n",
    "Dask also supports other network protocols such as [TLS](https://distributed.dask.org/en/stable/tls.html), [websockets](https://distributed.dask.org/en/stable/protocol.html) and [UCX](https://docs.rapids.ai/api/dask-cuda/nightly/examples/ucx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLS/SSL for secure communication\n",
    "\n",
    "Dask cluster components can use certificates to mutually authenticate and communicate securely if run in an untrusted envronment. You can either generate certificates for the scheduler, worker and client automatically and distribute those or you can generate temporary credentials. \n",
    "\n",
    "Some cluster managers such as `dask-cloudprovider` will automatically enable TLS and generate one-time certificates when exposing clusters to the internet from the public cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Scheduler, Worker, Client\n",
    "from distributed.security import Security\n",
    "\n",
    "security = Security.temporary()\n",
    "\n",
    "async with Scheduler(security=security) as scheduler:\n",
    "    async with Worker(scheduler.address, security=security) as worker:\n",
    "        async with Client(\n",
    "            scheduler.address, security=security, asynchronous=True\n",
    "        ) as client:\n",
    "            print(await client.submit(lambda: 1 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Websockets\n",
    "\n",
    "Dask can also communicate via websockets instead of TCP. There is a very small performance overhead to doing this but it means that the dashboard and communication happen on the same port and can be reverse proxied by a layer 7 proxy like nginx. This is necessary for some deployment scenarios where you cannot exprt ports but you can proxy web services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCX\n",
    "\n",
    "On systems with high performance networking such as Infiniband or NVLink Dask can also leverage [UCX](https://openucx.org/) which provides a unified communication protocol that automatically upgrades communication to use the fastest hardware available. This is vital for good performance on HPC systems with Infiniband or systems with multiple GPU workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "b7cfefc49ba37c014f46ba4939b08cd2bb5b25f27a21eb8109ce6441079573de"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dask-tutorial]",
   "language": "python",
   "name": "conda-env-.conda-dask-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
